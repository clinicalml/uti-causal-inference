{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration setup\n",
    "### TODO: remove unused functions, imports, variables and commented out cells\n",
    "### This requires a config.py file to run\n",
    "### Specify a treatment name among the options (\"alternatives\" & \"second_line\"), default is \"alternatives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify treatment\n",
    "# treatment_name = \"alternatives\"\n",
    "treatment_name = \"second_line\"\n",
    "in_name = \"manuscript_covariates_5_final\"\n",
    "\n",
    "treatment = treatment_name.capitalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying input and output of the function\n",
    "# in_name_1 = \"manuscript_covariates_final\"\n",
    "in_name_2 = f\"gen_uti_cohort-first-{treatment_name}\"\n",
    "in_name_3 = f\"cache_data_uti_omop_first_{treatment_name}\"\n",
    "\n",
    "# out_name_1 = f\"cohort_features_{treatment}_adverse_events_censor_v0.pkl\"\n",
    "# out_name_2 = f\"omop_1_year_features_{treatment}_adverse_events_censor_v0.pkl\"\n",
    "# out_name_3 = f\"cohort_features_{treatment}_treatment efficacy_censor_v0.pkl\"\n",
    "# out_name_4 = f\"omop_1_year_features_{treatment}_treatment efficacy_censor_v0.pkl\"\n",
    "\n",
    "#out_name_5... = all the grid search files in Logs/Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import resample\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import sparse\n",
    "import datetime\n",
    "import scipy.sparse\n",
    "from scipy.sparse import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "from __future__ import division\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import Utils.dbutils as dbutils\n",
    "import Utils.data_utils as data_utils\n",
    "import Generators.CohortGenerator as CohortGenerator\n",
    "import Generators.FeatureGenerator as FeatureGenerator\n",
    "import config\n",
    "import copy\n",
    "local_imports = (\n",
    "    dbutils,\n",
    "    data_utils,\n",
    "    CohortGenerator,\n",
    "    FeatureGenerator,\n",
    "    config\n",
    ")\n",
    "for i in local_imports:\n",
    "    i = importlib.reload(i)\n",
    "\n",
    "\n",
    "\n",
    "database_name = 'omop_v6'\n",
    "conn_string = \"dbname=\"+database_name + \" host=/var/run/postgresql\"\n",
    "conn=psycopg2.connect(conn_string)\n",
    "print('Connected!')\n",
    "database_name = config.DB_NAME\n",
    "print(database_name)\n",
    "config_path = 'postgresql://{database_name}'.format(\n",
    "    database_name = database_name\n",
    ")\n",
    "connect_args = {\"host\": '/var/run/postgresql/'} # connect_args to pass to sqlalchemy create_engine function\n",
    "\n",
    "# schemas \n",
    "schema_name = 'cdm_6871_21' # all created tables will be created using this schema\n",
    "cdm_schema_name = config.OMOP_CDM_SCHEMA # the name of the schema housing your OMOP CDM tables\n",
    "print(f\"cdm schema: {cdm_schema_name}\")\n",
    "# set up database, reset schemas as needed\n",
    "db = dbutils.Database(config_path, schema_name, connect_args, cdm_schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_cohort = pd.read_sql(f\"SELECT * FROM cdm_6871_21.{in_name}\", conn)\n",
    "new_cohort = new_cohort.loc[new_cohort.antibiotic_type.isin(['nitrofurantoin','trimethoprim-sulfamethoxazole',treatment_name])]\n",
    "#initialize feature dictionary\n",
    "feat_dict = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cohort.antibiotic_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run if using OMOP (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying cohort and feature config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omop_name = f'__uti_cohort_omop_first_{treatment_name}'\n",
    "cohort_script_path = config.SQL_PATH_COHORTS + f'/gen_uti_cohort-first-{treatment_name}.sql'\n",
    "\n",
    "# cohort parameters  \n",
    "params = {\n",
    "          'cohort_table_name'     : omop_name,\n",
    "          'schema_name'           : schema_name,\n",
    "          'aux_data_schema'       : config.CDM_AUX_SCHEMA,\n",
    "          'training_start_date'   : '2009-01-01',\n",
    "          'dummy_date'            : '1900-01-01'\n",
    "         }\n",
    "\n",
    "cohort = CohortGenerator.Cohort(\n",
    "    schema_name=schema_name,\n",
    "    cohort_table_name=omop_name,\n",
    "    cohort_generation_script=cohort_script_path,\n",
    "    cohort_generation_kwargs=params,\n",
    "    outcome_col_name='y'\n",
    ")\n",
    "\n",
    "featureSet = FeatureGenerator.FeatureSet(db)\n",
    "featureSet.add_default_features(\n",
    "    ['drugs_relative','conditions_relative','procedures_relative','specialty_relative'],\n",
    "    schema_name,\n",
    "    omop_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building cohort and omop features.  I have already built the table and the features so you can use replace=False flag, and from_cached=True to retrieve them assuming access to the  cache_data_path and cohort_script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cohort.build(db, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the Feature Set by executing SQL queries and reading into sparse matrices\n",
    "cache_data_path = f'/data/ibc_6871_21/ncjones_ibc/cache_data_uti_omop_first_{treatment_name}'\n",
    "featureSet.build(cohort, from_cached=True, cache_file=cache_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading Feature matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_filt, feature_matrix_3d_transpose, remap, good_feature_names = \\\n",
    "    FeatureGenerator.postprocess_feature_matrix(cohort, featureSet,training_end_date_col='dummy_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OMOP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#1 year\n",
    "\n",
    "feature_matrix_counts_1, feature_names_1 = data_utils.window_data(\n",
    "    window_lengths = [3,7,30,180,365],\n",
    "    feature_matrix = feature_matrix_3d_transpose,\n",
    "    all_feature_names = good_feature_names,\n",
    "    cohort = cohort,\n",
    "    featureSet = featureSet,\n",
    "    cohort_end_date_col='dummy_date'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add antibiotic feature to omop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#filtering for feature columns\n",
    "feature_dict = featureSet.__dict__['id_map_rev']\n",
    "feature_set = list(feature_dict.values())\n",
    "ids_to_exclude = [x for x in feature_set if x not in remap['id']]\n",
    "new_feature_dict = {k:v for k,v in feature_dict.items() if v not in ids_to_exclude}\n",
    "\n",
    "\n",
    "new_cohort = new_cohort.loc[new_cohort.condition_occurrence_id.isin(list(new_feature_dict.keys()))]\n",
    "new_cohort = new_cohort.set_index('condition_occurrence_id')\n",
    "new_cohort = new_cohort.reindex(list(new_feature_dict.keys()))\n",
    "new_cohort = new_cohort.reset_index()\n",
    "\n",
    "filtered_cohort = new_cohort.copy()\n",
    "filtered_cohort['treatment_'] = filtered_cohort.antibiotic_type == treatment_name\n",
    "\n",
    "#combining features\n",
    "\n",
    "combined_features_1 = vstack([feature_matrix_counts_1, scipy.sparse.csr_matrix(filtered_cohort['treatment_'])]).T\n",
    "combined_feature_names_1 = feature_names_1 + ['treatment_']\n",
    "\n",
    "\n",
    "#update feat_dict\n",
    "feat_dict['treatment'][\"omop_1_year_features\"] = (feature_matrix_counts_1.T, feature_names_1)\n",
    "feat_dict['censor'][\"omop_1_year_features\"] = (combined_features_1, combined_feature_names_1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of OMOP Portion (currently not optional because the dataframe size of new cohort changes when adding negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "filtered_cohort = new_cohort.copy()\n",
    "\n",
    "\n",
    "cols_to_exclude = [\n",
    "'no_previous_180_excluded_event',\n",
    "'no_previous_180_day_event',\n",
    "'no_two_previous_365_day_event',\n",
    " 'no_previous_excluded_event_ever',\n",
    "'previous_uti_condition_occurence_id',\n",
    " 'multi',\n",
    " 'year_of_birth',\n",
    "'level_0',\n",
    " 'index',\n",
    " 'condition_occurrence_id',\n",
    " 'person_id',\n",
    " 'condition_concept_id',\n",
    " 'condition_start_date',\n",
    " 'condition_start_datetime',\n",
    " 'condition_end_date',\n",
    " 'condition_end_datetime',\n",
    " 'visit_occurrence_id',\n",
    " 'visit_detail_id',\n",
    " 'drug_concept_id',\n",
    " 'drug_name',\n",
    " 'antibiotic_name',\n",
    " 'antibiotic_type',\n",
    " 'visit_provider_id',\n",
    " 'drug_exposure_id',\n",
    " 'drug_exposure_start_date',\n",
    " 'drug_exposure_start_datetime',\n",
    " 'provider_id',\n",
    " 'provider_name',\n",
    " 'npi',\n",
    " 'post_UTI_codes',\n",
    " 'recurrent_uti',\n",
    " 'first_uti',\n",
    " 'followup_time',\n",
    " 't_sum',\n",
    " 't_bin',\n",
    " 't_uti_bin',\n",
    " 't_neph_bin',\n",
    " 'AE_c_diff',\n",
    " 'AE_GI',\n",
    " 'AE_skin',\n",
    " 'AE_AKI',\n",
    " 'AE_other',\n",
    " 't_sepsis_sum',\n",
    " 't_i_sepsis_sum',\n",
    " 't_i_uti_sum',\n",
    " 't_i_neph_sum',\n",
    " 't_sepsis_bin',\n",
    " 't_i_uti_bin',\n",
    " 't_i_neph_bin',\n",
    " 't_i_sepsis_bin',\n",
    " 't_i_sum',\n",
    " 't_uti_sum',\n",
    " 't_neph_sum',\n",
    " 't_i_bin','AE_any','less_15','less_30','less_90','followup_time'] + \\\n",
    " [x for x in filtered_cohort.columns if 'full_condition_name' in x] + \\\n",
    " ['fibro_' + m + '_mon_outcome' for m in ['1','3','6']] + \\\n",
    " ['hernia_' + m + '_mon_outcome' for m in ['1','3','6']] + \\\n",
    " ['fracture_' + m + '_mon_outcome' for m in ['1','3','6']]\n",
    "cohort_features = filtered_cohort.drop(columns=cols_to_exclude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create outcome and feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = {'treatment':{'first_line':(1*(filtered_cohort.antibiotic_type==\"nitrofurantoin\") + 1*(filtered_cohort.antibiotic_type==\"trimethoprim-sulfamethoxazole\")),\n",
    "                             'second_line':1*filtered_cohort.antibiotic_type=='second_line',\n",
    "                             'alternatives':1*filtered_cohort.antibiotic_type=='alternatives'},\n",
    "                'censor' : {'15':1*filtered_cohort.less_15==0,\n",
    "                                 '30':1*filtered_cohort.less_30==0,\n",
    "                                 '90':1*filtered_cohort.less_90==0}\n",
    "                    \n",
    "}\n",
    "\n",
    "\n",
    "mod_cohort_features = cohort_features.copy()\n",
    "mod_cohort_features['treatment_'] = 1*(filtered_cohort.antibiotic_type==treatment_name)\n",
    "\n",
    "\n",
    "#update feature dictionary\n",
    "feat_dict['treatment'][\"cohort_features\"] = (scipy.sparse.csr_matrix(cohort_features.values), list(cohort_features.columns))\n",
    "feat_dict['censor'][\"cohort_features\"] =  (scipy.sparse.csr_matrix(mod_cohort_features.values), list(mod_cohort_features.columns))\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Variable Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treatment variable shapes\")\n",
    "print(\"features\")\n",
    "for key, value in feat_dict[\"treatment\"].items():\n",
    "    print(key, value[0].shape)\n",
    "print(\"outcomes\")\n",
    "for key, value in outcome_dict[\"treatment\"].items():\n",
    "    print(key, value.shape)\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nCensor variable shapes\")\n",
    "print(\"features\")\n",
    "for key, value in feat_dict[\"censor\"].items():\n",
    "    print(key, value[0].shape)\n",
    "\n",
    "print(\"outcomes\")\n",
    "for key, value in outcome_dict[\"censor\"].items():\n",
    "    print(key, value.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the variable shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grid Search and plotting Helper Functions\n",
    "def run_logreg(matrix : scipy.sparse.csr.csr_matrix, y, CV=False,folds=3,params=None,search=False,add_propensity=False):\n",
    "    sparse_features = scipy.sparse.csr_matrix(matrix)\n",
    "    X, T = sparse_features, y.values\n",
    "    # build model for p(t=1|x); prob of getting first-line antibiotic \n",
    "    X_Train, X_Test, T_Train, T_Test = train_test_split(X, T, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    X_Dev, X_Val, T_Dev, T_Val = train_test_split(X_Train, T_Train, test_size=0.25, random_state=0)\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    X_Train = scaler.fit_transform(X_Train)\n",
    "    X_Test = scaler.transform(X_Test)\n",
    "    \n",
    "    X_Dev = scaler.fit_transform(X_Dev)\n",
    "    X_Val = scaler.transform(X_Val)\n",
    "\n",
    "    if search:\n",
    "        C=[.01,.1,.25,.5,1]\n",
    "        penalty=['l1','l2']\n",
    "        params_names = [\"C\",\"penalty\"]\n",
    "\n",
    "        if not CV:\n",
    "            best_params, best_score = [0.1, 'l1'], float('-inf')\n",
    "            for c in C:\n",
    "                for p in penalty:\n",
    "                    model = LogisticRegression(random_state=0, penalty=p, C=c, solver='liblinear')\n",
    "                    model.fit(X_Dev, T_Dev)\n",
    "                    s = roc_auc_score(T_Val, model.predict_proba(X_Val)[:,1])\n",
    "                    # print(c,p,s)\n",
    "                    if s > best_score:\n",
    "                        best_params, best_score = [c,p], s\n",
    "\n",
    "            classifier = LogisticRegression(random_state=0, penalty=best_params[1], C=best_params[0], solver='liblinear')\n",
    "            best_params = dict(zip(params_names,best_params))\n",
    "        else:\n",
    "            clf = GridSearchCV(LogisticRegression(C=0.25, penalty='l1',solver='liblinear', random_state=0,verbose=0),\n",
    "                               param_grid={'C':C,'penalty':penalty},cv=folds,scoring=\"roc_auc\",refit=True\n",
    "                         )\n",
    "            clf.fit(X_Train,T_Train)\n",
    "            classifier = clf.best_estimator_\n",
    "            best_params = clf.best_params_\n",
    "            best_score = clf.best_score_\n",
    "    else:\n",
    "        classifier = LogisticRegression(random_state=0, penalty=params['penalty'], C=params['C'], solver=params['solver'])\n",
    "        best_params = params\n",
    "        best_score = None\n",
    "    classifier.fit(X_Train, T_Train)\n",
    "\n",
    "    T_Pred = classifier.predict(X_Test)\n",
    "    T_Pred_Prob = classifier.predict_proba(X_Test)\n",
    "#     print(\"Accuracy:\", np.round(metrics.accuracy_score(T_Test, T_Pred), 4))\n",
    "\n",
    "\n",
    "    acc = np.round(metrics.accuracy_score(T_Test, T_Pred), 4)\n",
    "    fpr, tpr, threshold = (metrics.roc_curve(T_Test, T_Pred_Prob[:,1]))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    if add_propensity:\n",
    "        cohort = filtered_cohort.copy()\n",
    "        classifier.fit(X, T)\n",
    "        cohort['propensity_score'] = np.clip(classifier.predict_proba(X)[:,1], 0.025, 0.975)\n",
    "        cohort['treatment'] = y\n",
    "        return cohort, classifier, best_params, best_score, (acc,roc_auc,fpr, tpr)\n",
    "    else:\n",
    "        return classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "#modified for omop\n",
    "def run_rf(matrix, y, CV=False,folds=3,search=False,params=None,add_propensity=False):\n",
    "    sparse_features = scipy.sparse.csr_matrix(matrix)\n",
    "    X, T = sparse_features, y.values\n",
    "    # build model for p(t=1|x); prob of getting first-line antibiotic \n",
    "    X_Train, X_Test, T_Train, T_Test = train_test_split(X, T, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    X_Dev, X_Val, T_Dev, T_Val = train_test_split(X_Train, T_Train, test_size=0.25, random_state=0)\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    X_Train = scaler.fit_transform(X_Train)\n",
    "    X_Test = scaler.transform(X_Test)\n",
    "    \n",
    "    X_Dev = scaler.fit_transform(X_Dev)\n",
    "    X_Val = scaler.transform(X_Val)\n",
    "\n",
    "    max_features=['auto', 'sqrt', 'log2']\n",
    "    n_estimators=[60, 80, 100, 120, 140]\n",
    "    min_samples_leaf=[25, 50, 75]\n",
    "    params_names = [\"max_features\",\"n_estimators\",\"min_samples\"]\n",
    "    \n",
    "    \n",
    "    if search:\n",
    "        if not CV:\n",
    "            best_params, best_score = [max_features[0], n_estimators[0], min_samples_leaf[0]], float('-inf')\n",
    "\n",
    "            for f in max_features:\n",
    "                for e in n_estimators:\n",
    "                    for l in min_samples_leaf:\n",
    "                        model=RandomForestClassifier(random_state=0, n_estimators = e, max_features = f, min_samples_leaf = l)\n",
    "                        model.fit(X_Dev, T_Dev)\n",
    "                        s = roc_auc_score(T_Val, model.predict_proba(X_Val)[:,1])\n",
    "                        # print(c,p,s)\n",
    "                        if s > best_score:\n",
    "                            best_params, best_score = [f, e, l], s\n",
    "\n",
    "\n",
    "            # #Train the model using the training sets y_pred=model.predict(X_test)\n",
    "            classifier = RandomForestClassifier(random_state=0, max_features=best_params[0],n_estimators=best_params[1],min_samples_leaf=best_params[2])\n",
    "            best_params = dict(zip(params_names,best_params))\n",
    "        else:\n",
    "            clf = GridSearchCV(RandomForestClassifier(n_estimators=100, max_features=\"auto\",min_samples_leaf=50, random_state=0),\n",
    "                               param_grid={'max_features':max_features,'n_estimators':n_estimators, \"min_samples_leaf\":min_samples_leaf},cv=folds,scoring=\"roc_auc\",refit=True\n",
    "                         )\n",
    "            clf.fit(X_Train,T_Train)\n",
    "            classifier = clf.best_estimator_\n",
    "            best_params = clf.best_params_\n",
    "            best_score = clf.best_score_\n",
    "    else:\n",
    "        classifier = RandomForestClassifier(random_state=0, n_estimators=params['n_estimators'], max_features=params['max_features'], min_samples_leaf=params['min_samples_leaf'])\n",
    "        best_params = params\n",
    "        best_score = None\n",
    "\n",
    "    classifier.fit(X_Train, T_Train)\n",
    "\n",
    "    T_Pred = classifier.predict(X_Test)\n",
    "    T_Pred_Prob = classifier.predict_proba(X_Test)\n",
    "#     print(\"Accuracy:\", np.round(metrics.accuracy_score(T_Test, T_Pred), 4))\n",
    "\n",
    "    acc = np.round(metrics.accuracy_score(T_Test, T_Pred), 4)\n",
    "    fpr, tpr, threshold = (metrics.roc_curve(T_Test, T_Pred_Prob[:,1]))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    \n",
    "    if add_propensity:\n",
    "        cohort = filtered_cohort.copy()\n",
    "        classifier.fit(X, T)\n",
    "        cohort['propensity_score'] = np.clip(classifier.predict_proba(X)[:,1], 0.025, 0.975)\n",
    "        cohort['treatment'] = y\n",
    "        return cohort, classifier, best_params, best_score, (acc,roc_auc,fpr, tpr)\n",
    "    else:\n",
    "        return classifier\n",
    "\n",
    "def run_lgbm(matrix, y, add_propensity=False,CV=False,folds=3,search=True,params=None):\n",
    "    sparse_features = scipy.sparse.csr_matrix(matrix)\n",
    "    X, T = sparse_features, y.values\n",
    "    # build model for p(t=1|x); prob of getting first-line antibiotic \n",
    "\n",
    "    X_Train, X_Test, T_Train, T_Test = train_test_split(X, T, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    X_Dev, X_Val, T_Dev, T_Val = train_test_split(X_Train, T_Train, test_size=0.25, random_state=0)\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    X_Train = scaler.fit_transform(X_Train)\n",
    "    X_Test = scaler.transform(X_Test)\n",
    "    \n",
    "    X_Dev = scaler.fit_transform(X_Dev)\n",
    "    X_Val = scaler.transform(X_Val)\n",
    "\n",
    "    boosting_type=[\"gbdt\",\"dart\"]\n",
    "    if not search:\n",
    "        learning_rates=[ .05]\n",
    "        num_leaves=[50]\n",
    "        if params:\n",
    "            lr = params[\"learning_rate\"]\n",
    "            num_leaves = params[\"num_leaves\"]\n",
    "            classifier = lgb.LGBMClassifier(random_state=0,learning_rate=lr,num_leaves=num_leaves)\n",
    "            best_params = params\n",
    "            best_score = None\n",
    "\n",
    "    else:\n",
    "        learning_rates=[.001,.005, .01, .05, .1]\n",
    "        num_leaves=[10,50,100,250]\n",
    "        params_names = [\"learning_rate\",\"num_leaves\"]\n",
    "    \n",
    "        if not CV:\n",
    "            best_params, best_score = [learning_rates[0], num_leaves[0]], float('-inf')\n",
    "\n",
    "            for l in learning_rates:\n",
    "                for n in num_leaves:\n",
    "                    model = lgb.LGBMClassifier(num_leaves=n, learning_rate=l,random_state=0)\n",
    "                    model.fit(X_Dev, T_Dev)\n",
    "                    s = roc_auc_score(T_Val, model.predict_proba(X_Val)[:,1])\n",
    "                    # print(c,p,s)\n",
    "                    if s > best_score:\n",
    "                        best_params, best_score = [l, n], s\n",
    "\n",
    "\n",
    "            # #Train the model using the training sets y_pred=model.predict(X_test)\n",
    "            classifier = lgb.LGBMClassifier(random_state=0,learning_rate=best_params[0],num_leaves=best_params[1])\n",
    "            best_params = dict(zip(params_names,best_params))\n",
    "            best_params[\"boosting_type\"] = 'gbdt'\n",
    "        else:\n",
    "            clf = GridSearchCV(lgb.LGBMClassifier(boosting_type=\"gbdt\", learning_rate=.01,num_leaves=50, random_state=0),\n",
    "                               param_grid={'boosting_type':boosting_type,'learning_rate':learning_rates, \"num_leaves\":num_leaves},cv=folds,scoring=\"roc_auc\",refit=True\n",
    "                         )\n",
    "            clf.fit(X_Train,T_Train)\n",
    "            classifier = clf.best_estimator_\n",
    "            best_params = clf.best_params_\n",
    "            best_score = clf.best_score_        \n",
    "\n",
    "    classifier.fit(X_Train, T_Train)\n",
    "\n",
    "    T_Pred = classifier.predict(X_Test)\n",
    "    T_Pred_Prob = classifier.predict_proba(X_Test)\n",
    "#     print(\"Accuracy:\", np.round(metrics.accuracy_score(T_Test, T_Pred), 4))\n",
    "\n",
    "\n",
    "    acc = np.round(metrics.accuracy_score(T_Test, T_Pred), 4)\n",
    "    fpr, tpr, threshold = (metrics.roc_curve(T_Test, T_Pred_Prob[:,1]))\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "    if add_propensity:\n",
    "        cohort = filtered_cohort.copy()\n",
    "        classifier.fit(X, T)\n",
    "        cohort['propensity_score'] = np.clip(classifier.predict_proba(X)[:,1], 0.025, 0.975)\n",
    "        cohort['treatment'] = y\n",
    "#         param_dictionary = dict(zip(params_names,best_params))\n",
    "#         param_dictionary[\"boosting_type\"] = \"gbdt\"\n",
    "\n",
    "\n",
    "        return cohort, classifier, best_params, best_score, (acc,roc_auc,fpr, tpr)\n",
    "    else:\n",
    "        return classifier\n",
    "    \n",
    "def compute_roc_auc(auc_metrics, name=None, show=None):\n",
    "    roc_auc, fpr, tpr = auc_metrics\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    if name:\n",
    "        plt.savefig(name)\n",
    "    if not show:\n",
    "        plt.close()\n",
    "\n",
    "def build_model(feature,outcome,y,model,cv=False,folds=3):\n",
    "    #assumes the functions can actually generate these things\n",
    "    if model == 1:\n",
    "        cohort, clf, params, scores, metric = run_logreg(feat_dict[outcome][feature][0],y,CV=cv,folds=folds,add_propensity=True)\n",
    "        return cohort, clf, params, scores, metric\n",
    "    elif model == 2:\n",
    "        cohort, clf, params, scores, metric = run_rf(feat_dict[outcome][feature][0],y,CV=cv,folds=folds,add_propensity=True)\n",
    "        return cohort, clf, params, scores, metric\n",
    "    elif model == 3:\n",
    "        cohort, clf, params, scores, metric = run_lgbm(feat_dict[outcome][feature][0],y,CV=cv,folds=folds,add_propensity=True)\n",
    "        return cohort, clf, params, scores, metric\n",
    "        \n",
    "def reweighting(cohort,name=None,show=False):\n",
    "    '''\n",
    "    Input: cohort with a propensity column named propensity_score and antibiotic type column\n",
    "           treatment name\n",
    "    \n",
    "    return plot'''\n",
    "    first_line, non_first_line = cohort[cohort['treatment']==1], cohort[cohort['treatment']==0]\n",
    "    n = cohort.shape[0]\n",
    "\n",
    "    age_range = np.arange(int(cohort.age.min()), int(cohort.age.max()+1))\n",
    "    counts1, counts0 = [], []\n",
    "    for a in age_range:\n",
    "        counts1.append(sum(1/first_line[first_line.age==a].propensity_score))\n",
    "        counts0.append(sum(1/(1-non_first_line[non_first_line.age==a].propensity_score)))\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 5))\n",
    "    # plt.rcParams[\"figure.figsize\"] = [7.50, 5.50]\n",
    "    # plt.rcParams[\"figure.autolayout\"] = True\n",
    "    axs[0].fill_between(age_range, counts1, step=\"pre\", alpha=0.5, label='First-line')\n",
    "    axs[0].fill_between(age_range, counts0, step=\"pre\", alpha=0.5, label=treatment)\n",
    "\n",
    "    # plt.plot(range(18,100),counts1, drawstyle=\"steps\")\n",
    "    # plt.plot(range(18,100),counts0, drawstyle=\"steps\")\n",
    "    axs[0].set_title(\"Age distribution reweighted by propensity scores\", size=13)\n",
    "\n",
    "    axs[1].hist(first_line.age, bins=88, alpha=0.5, label=\"First-line\");\n",
    "    axs[1].hist(non_first_line.age, bins=88, alpha=0.5, label=treatment)\n",
    "    axs[1].set_title(\"Age distribution before reweighting\", size=13)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"Age\", size=11)\n",
    "        ax.set_ylabel(\"Count\", size=11)\n",
    "        ax.legend()\n",
    "\n",
    "    avg1 = np.average(age_range, weights=counts1)\n",
    "    avg0 = np.average(age_range, weights=counts0)\n",
    "    avg_all = np.average(age_range, weights=np.array(counts1)+np.array(counts0))\n",
    "    sigma = np.sqrt(np.average((age_range-avg_all)**2, weights=np.array(counts1)+np.array(counts0)))\n",
    "    age_diff = (avg1-avg0)/sigma\n",
    "    \n",
    "    if name:\n",
    "        plt.savefig(name)\n",
    "    if not show:\n",
    "        plt.close()\n",
    "    return fig, age_diff\n",
    "    \n",
    "     \n",
    "def propensity(cohort,name=None,show=False):\n",
    "    '''\n",
    "    Input: cohort with a propensity column named propensity_score and antibiotic type column\n",
    "           treatment name\n",
    "    \n",
    "    return plot'''\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    t1 = cohort[cohort['treatment']==1].propensity_score\n",
    "    t2 = cohort[cohort['treatment']==0].propensity_score\n",
    "    weight1 = np.ones_like(t1) / len(t1)\n",
    "    weight2 = np.ones_like(t2) / len(t2)\n",
    "\n",
    "    plt.hist(t1, weights=weight1, bins=50, alpha=0.5, label=\"First line (T=1)\")\n",
    "    plt.hist(t2, weights=weight2, bins=50, alpha=0.5, label=f\"{treatment} (T=0)\")\n",
    "\n",
    "    plt.xlabel(\"Propensity score\", size=14)\n",
    "    plt.ylabel(\"Count\", size=14)\n",
    "    plt.title(f\"Distribution of Propensity Scores (First line vs {treatment})\")\n",
    "    plt.legend(loc='upper right')\n",
    "    # plt.savefig(\"overlapping_histograms_with_matplotlib_Python.png\")\n",
    "    if name:\n",
    "        plt.savefig(name)\n",
    "    if not show:\n",
    "        plt.close()\n",
    "    return plt\n",
    "    \n",
    "def calibration(cohort,name=None,show=False):\n",
    "    '''\n",
    "    Input: cohort with a propensity column named propensity_score and antibiotic type column\n",
    "           treatment name\n",
    "    \n",
    "    return plot'''\n",
    "   \n",
    "    bins = [np.quantile(cohort.propensity_score, x/8) for x in range(9)]\n",
    "    df_cali = pd.DataFrame(columns=['pred', 'actual', 'errorbar'])\n",
    "    for i in range(8):\n",
    "        temp = cohort[(cohort.propensity_score>=bins[i]) & (cohort.propensity_score<=bins[i+1])]\n",
    "        x = np.mean(temp.propensity_score)\n",
    "        y = len(temp[temp['treatment']==1])/len(temp)\n",
    "        std = np.std(temp.propensity_score)\n",
    "        df_cali.loc[len(df_cali)] = [x,y,1.96*std]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    # plt.plot(df_cali.pred, df_cali.actual, '-o')\n",
    "    plt.errorbar(df_cali.pred, df_cali.actual, yerr=df_cali.errorbar, fmt='o', ls='--')\n",
    "    plt.plot([0, 1], [0, 1], ls='--')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Plot for '+treatment)\n",
    "    if name:\n",
    "        plt.savefig(name)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "import pathlib\n",
    "def generate_plots(model_config=None,model=None, outcome=None,build=False, show_results=False,log_name=\"logs\"):\n",
    "    if outcome == None:\n",
    "        print(RaiseExceptionError(\"Outcome variable not specified\"))\n",
    "    feat_name = model_config[\"name\"]\n",
    "    #directory: treatment/outcome variable/feature/\n",
    "    dir_name = f\"Logs_2023/Grid search/{log_name}_{treatment.lower()}/{model_config['outcome']}/{model_config['feature']}/\"\n",
    "    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    f_name =  dir_name + feat_name\n",
    "\n",
    "\n",
    "    \n",
    "    if build and model_config==None:\n",
    "        cohort, clf, params, score, metrics = build_model(featureset,outcome,model)\n",
    "    else:\n",
    "        model = model_config[\"model_num\"]\n",
    "        cohort = model_config[\"cohort\"]\n",
    "        clf = model_config[\"clf\"]\n",
    "        params = model_config[\"params\"]\n",
    "        score = model_config[\"score\"]\n",
    "        metrics = model_config[\"metrics\"]\n",
    "        accuracy, roc_auc, fpr, tpr = metrics\n",
    "        roc_auc_metrics = (roc_auc, fpr, tpr)\n",
    "        featureset = model_config[\"feature\"]\n",
    "        params = model_config[\"params\"]\n",
    "    model_index = model-1\n",
    "    if model == 1:\n",
    "        importances = clf.coef_[0]\n",
    "    else:\n",
    "        importances = clf.feature_importances_\n",
    "    \n",
    "    imp_name =  feat_imp[model_index]\n",
    "    df_coef = pd.DataFrame(list(zip(feat_dict[outcome][featureset][1], importances)), columns=['feature',imp_name])\n",
    "    feat_pd = df_coef.iloc[(-df_coef[imp_name].abs()).argsort()].head(100)\n",
    "    feat_pd.to_csv(f_name + \"_top_features.csv\")\n",
    "\n",
    "    if show_results:\n",
    "        show=True\n",
    "    else:\n",
    "        show=False\n",
    "        \n",
    "    acc_pd = pd.DataFrame(np.array([[accuracy],[roc_auc]]).reshape(1,2),columns=[\"accuracy\",\"roc_auc\"])\n",
    "    acc_pd.to_csv(f_name + \"_metrics.csv\")\n",
    "\n",
    "    auc_name = f_name + \"_roc_auc.png\"\n",
    "    compute_roc_auc(roc_auc_metrics,name=auc_name,show=show)\n",
    "    \n",
    "    r_name = f_name + \"_reweighting.png\"\n",
    "    val, age_diff = reweighting(cohort,name=r_name,show=show)\n",
    "    age_diff_pd = pd.DataFrame([age_diff], columns=[\"standardized age difference\"])\n",
    "    \n",
    "    age_diff_pd.to_csv(f_name + \"_age_diff.csv\")\n",
    "    \n",
    "    p_name = f_name + \"_propensity.png\"\n",
    "    p_plot = propensity(cohort,name=p_name,show=show)\n",
    "    \n",
    "    c_name = f_name + \"_calibration.png\"\n",
    "    c_plot = calibration(cohort,name=c_name,show=show)\n",
    "    \n",
    "    \n",
    "    param_df = pd.DataFrame(np.array(list(params.values())).reshape(1,len(params.values())),columns=list(params.keys()))\n",
    "    param_df.to_csv(f_name + \"_best_params.csv\")\n",
    "    \n",
    "    if show_results:\n",
    "        print(featureset.upper(), f\"BEST MODEL ({model_list[model_index].upper()})\")\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"ROC_AUC: {roc_auc:.4}\")\n",
    "        print(f\"Best params: {params}\")\n",
    "        print(f\"feat_size {df_coef.shape}\")\n",
    "        print(\"feature_importances\")\n",
    "        display(feat_pd.head(30))\n",
    "        print(\"age reweighted standardized difference: \",age_diff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameter Tuning Models (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check we're getting both features\n",
    "grid_params ={'censor' : {'outcome_names':['15','30','90'],'feats':feat_dict['censor']},\n",
    "              'treatment' : {'outcome_names':[treatment_name],'feats': feat_dict['treatment']}}\n",
    "\n",
    "\n",
    "#NOAH ADDED NEW GRID SEARCH FUNCTIONALITY FOR OUTCOME\n",
    "outcomes = ['censor','treatment']\n",
    "for outcome in outcomes:\n",
    "\n",
    "    grid_features= list(grid_params[outcome]['feats'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#choose the parameter model combo\n",
    "\n",
    "#change grid_params if grid searching over treatment outcome instead of censor outcome\n",
    "\n",
    "all_best_hyperparams = {}\n",
    "grid_params ={'censor' : {'outcome_names':['15','30','90'],'feats':feat_dict['censor']},\n",
    "              'treatment' : {'outcome_names':[treatment_name],'feats': feat_dict['treatment']}}\n",
    "\n",
    "\n",
    "#NOAH ADDED NEW GRID SEARCH FUNCTIONALITY FOR OUTCOME\n",
    "outcomes = ['censor','treatment']\n",
    "for outcome in outcomes:\n",
    "\n",
    "    grid_features= list(grid_params[outcome]['feats'].keys())\n",
    "    grid_outcomes = [(k,v) for k,v in outcome_dict[outcome].items() if k in grid_params[outcome]['outcome_names']]\n",
    "\n",
    "\n",
    "    feat_imp = [\"Coefficient\",\"Gini importance\",\"Number of splits\"]\n",
    "    model_list = [\"Logistic_Regression\",\"Random_Forest\",\"LGBM\"]\n",
    "\n",
    "    best_models = []\n",
    "    all_models = []\n",
    "    for out_name, out_val in grid_outcomes:\n",
    "        y = out_val\n",
    "        for f in grid_features:\n",
    "            best_score = -float(\"inf\")\n",
    "            for m in range(1,4):\n",
    "                #add outcome_name to model feature dictionary\n",
    "                cohort, clf, params, score, metric = build_model(f,outcome,y,m,cv=True,folds=3)\n",
    "                model_feature = {\"name\":f\"{model_list[m-1]}\",\"outcome\":out_name, \"score\":score,\"feature\":f,\"model_num\":m,\"metrics\":metric,\"params\":params,\"clf\":clf,\"cohort\":cohort}\n",
    "                all_models.append(model_feature)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_model_num = m\n",
    "                    best_params = params\n",
    "                    best_metric = metric\n",
    "                    best_clf = clf\n",
    "                    best_cohort = cohort\n",
    "\n",
    "            print(f\"{f} with {out_name} {outcome} outcome models DONE.\")\n",
    "            print(\"BEST MODEL WAS \", f\"{model_list[best_model_num-1]} with ROC_AUC score: {best_score}\")\n",
    "            print(f\"Best params are: {best_params}\")\n",
    "            best_model_feature = {\"name\":f\"{model_list[best_model_num-1]}\",\"score\":best_score,\"feature\":f,\"model_num\":best_model_num,\"metrics\":best_metric,\"params\":best_params,\"clf\":best_clf,\"cohort\":best_cohort}\n",
    "            best_models.append(best_model_feature)\n",
    "            \n",
    "            #loading the best params into dictionary\n",
    "            best_params['clf'] = model_list[best_model_num-1]\n",
    "            all_best_hyperparams.setdefault(f, {}).setdefault(treatment_name, {})[out_name] = best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_best_hyperparams,open(f'best_{treatment_name}_hyperparameters.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we convert the clfs\n",
    "\n",
    "## then we separate the dictionaries into the variables of interest using a dict comprehension\n",
    "# first these: best_alternatives_cohort, best_second_line_cohort, best_second_line_omop, best_alternatives_omop\n",
    "# then create these: best_D_params_cohort, best_A_params_cohort, best_D_params_omop, best_A_params_omop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_log_name = f\"logs_{in_name}\"\n",
    "for outcome in outcomes:\n",
    "    for model_feature_config in all_models:\n",
    "        generate_plots(model_config=model_feature_config,outcome=outcome,show_results=False,log_name=new_log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATE Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATE helper functions\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# define function that computes the IPTW estimator\n",
    "def run_ps(df, y, features=None, params=None):\n",
    "#     estimate the propensity score\n",
    "\n",
    "    X = scipy.sparse.csr_matrix(df[features])\n",
    "    T = df['treatment']\n",
    "    X = MaxAbsScaler().fit_transform(X) #change to maxabs scaling\n",
    "    ps = LogisticRegression(C=0.1, penalty='l1', solver='liblinear',tol=1e-4, max_iter=75).fit(X, T).predict_proba(X)[:, 1]\n",
    "\n",
    "    weight = (T-ps) / (ps*(1-ps)) # define the weights\n",
    "    y_ = df[y]\n",
    "    z = df['treatment']\n",
    "    \n",
    "    df[\"ips\"] = np.where(df['treatment'] == 1, 1 / ps, 1 / (1 - ps))\n",
    "    df['ipsw'] = df[y]*df['ips']\n",
    "    \n",
    "    ey1 = z*y_/ps / sum(z/ps)\n",
    "    ey0 = (1-z)*y_/(1-ps) / sum((1-z)/(1-ps))\n",
    "    ate = ey1.sum()-ey0.sum()\n",
    "    return [ate, ey0.sum(), ey1.sum()] # compute the ATE\n",
    "\n",
    "\n",
    "def bootstrap(test, bootstrap_sample = 1, other_word = '', features=None, params=None):\n",
    "\n",
    "    out = Parallel(n_jobs=15)(delayed(run_ps)(test.sample(frac=1, replace=True), y, features=features, params=params)\n",
    "                              for _ in range(bootstrap_sample))\n",
    "\n",
    "    #ates = np.array(ates)\n",
    "    ates = np.array(out)[:,0]\n",
    "    y0s = np.array(out)[:,1]\n",
    "    y1s = np.array(out)[:,2]\n",
    "    \n",
    "    EY1 = np.round(y1s.mean(),4); EY0 = np.round(y0s.mean(),4)\n",
    "    ATE = np.round(ates.mean(),4)\n",
    "    CI95 = (np.round(np.percentile(ates, 2.5),4), np.round(np.percentile(ates, 97.5),4))\n",
    "    \n",
    "    # print \"Showing results for\", y, other_word\n",
    "    print(\"ATE for\", other_word, \":\", ATE)\n",
    "    print(\"95% C.I.: (\", CI95[0], \", \", CI95[1], \")\")\n",
    "    print(\"EY1:\", EY1)\n",
    "    print(\"EY0:\", EY0)\n",
    "    return [EY1, EY0, ATE, CI95]\n",
    "\n",
    "def run_ps2(data, params, verbose=True):\n",
    "    X, T, y_ = data\n",
    "    if verbose == True:\n",
    "        print(f\"Feature shape {X.shape}, Treatment shape {T.shape}, Outcome shape {y_.shape}\")\n",
    "        \n",
    "    X = MaxAbsScaler().fit_transform(X) #change to maxabs scaling\n",
    "    ps = lgb.LGBMClassifier().set_params(**params).fit(X, T).predict_proba(X)[:, 1]\n",
    "    \n",
    "    weight = (T-ps) / (ps*(1-ps)) # define the weights\n",
    "    ey1 = T*y_/ps / sum(T/ps)\n",
    "    ey0 = (1-T)*y_/(1-ps) / sum((1-T)/(1-ps))\n",
    "    ate = ey1.sum()-ey0.sum()\n",
    "    return [ate, ey0.sum(), ey1.sum()] # compute the ATE\n",
    "\n",
    "    \n",
    "def sparse_bootstrap(X_sparse, T, y, bootstrap_sample = 1, other_word = '', params=None):\n",
    "    out = Parallel(n_jobs=15)(delayed(run_ps2)(resample(X_sparse, T, filtered_cohort[y]), params=params)\n",
    "                              for _ in range(bootstrap_sample))\n",
    "    #ates = np.array(ates)\n",
    "    ates = np.array(out)[:,0]\n",
    "    y0s = np.array(out)[:,1]\n",
    "    y1s = np.array(out)[:,2]\n",
    "    \n",
    "    EY1 = np.round(y1s.mean(),4); EY0 = np.round(y0s.mean(),4)\n",
    "    ATE = np.round(ates.mean(),4)\n",
    "    CI95 = (np.round(np.percentile(ates, 2.5),4), np.round(np.percentile(ates, 97.5),4))\n",
    "    \n",
    "    # print \"Showing results for\", y, other_word\n",
    "    print(\"ATE for\", other_word, \":\", ATE)\n",
    "    print(\"95% C.I.: (\", CI95[0], \", \", CI95[1], \")\")\n",
    "    print(\"EY1:\", EY1)\n",
    "    print(\"EY0:\", EY0)\n",
    "    return [EY1, EY0, ATE, CI95]\n",
    "\n",
    "def censor_bootstrap(X_sparse, T, y, D, bootstrap_sample = 1, other_word = '', params:tuple=None):\n",
    "    \n",
    "    out = Parallel(n_jobs=15)(delayed(run_ps3)(resample(X_sparse, T, filtered_cohort[y], D), params=params)\n",
    "                              for _ in range(bootstrap_sample))\n",
    "    #ates = np.array(ates)\n",
    "    ates = np.array(out)[:,0]\n",
    "    y0s = np.array(out)[:,1]\n",
    "    y1s = np.array(out)[:,2]\n",
    "    \n",
    "    EY1 = np.round(y1s.mean(),6); EY0 = np.round(y0s.mean(),6)\n",
    "    ATE = np.round(ates.mean(),6)\n",
    "    CI95 = (np.round(np.percentile(ates, 2.5),6), np.round(np.percentile(ates, 97.5),6))\n",
    "    \n",
    "    # print \"Showing results for\", y, other_word\n",
    "    print(\"ATE for\", other_word, \":\", ATE)\n",
    "    print(\"95% C.I.: (\", CI95[0], \", \", CI95[1], \")\")\n",
    "    print(\"EY1:\", EY1)\n",
    "    print(\"EY0:\", EY0)\n",
    "    return [EY1, EY0, ATE, CI95]\n",
    "\n",
    "def run_ps3(data, params, verbose=False, prop = False):\n",
    "    #computes the IPTW estimator taking into account censoring\n",
    "    params_copy = copy.deepcopy(params)\n",
    "    A_params, D_params = params_copy\n",
    "    clf_A = A_params.pop('clf', None)\n",
    "    clf_D = D_params.pop('clf', None)\n",
    "\n",
    "    #check to make sure a classifier was provided\n",
    "    assert not isinstance(clf_A,type(None)), \"no treatment classifier provided\"\n",
    "    assert not isinstance(clf_D,type(None)), \"no censor classifier provided\"\n",
    "    \n",
    "\n",
    "    # D = 1 if followed for at least 30 days and 0 if less than 30 days\n",
    "    # A = 1 if given second line and 0 if given first line\n",
    "    # X are the covariates\n",
    "    # Y is the outcome (which is treated as missing when D = 0)\n",
    "    # (1) fit propensity model P(A=1| X)\n",
    "    # (2) fit model for non-missing P(D=1 | X, A)\n",
    "    # The estimated ATE for this bootstrap sample is then, averaging over all patients with D=1\n",
    "    # E[ A_i Y_i / [P(A=1 | X_i) P(D=1 | A, X_i)] - (1-A_i) Y_i / [P(A=0 | X_i) P(D=1 | A, X_i)] ]\n",
    "\n",
    "    X, A, Y, D = data\n",
    "    if verbose == True:\n",
    "        print(f\"Feature shape {X.shape}, Treatment shape {A.shape}, Outcome shape {Y.shape}, D shape {D.shape}\")\n",
    "    \n",
    "\n",
    "    #just the X variable\n",
    "    X_1 = MaxAbsScaler().fit_transform(X) #change to maxabs scaling\n",
    "    #the X and A variables\n",
    "\n",
    "    X_2 = vstack([X_1.T, scipy.sparse.csr_matrix(A)]).T\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #fit propensity model\n",
    "    ps = clf_A.set_params(**A_params).fit(X_1, A).predict_proba(scipy.sparse.csr_matrix(X_1))[:, 1]\n",
    "    if prop:\n",
    "        return(ps)\n",
    "    else:\n",
    "        #fit model for non-missing D\n",
    "        ps_d = clf_D.set_params(**D_params).fit(X_2, D).predict_proba(scipy.sparse.csr_matrix(X_2))[:, 1]\n",
    "\n",
    "        #filter for patients with D=1\n",
    "        ps_d_filtered = ps_d[D==1]\n",
    "        ps_filtered = ps[D==1]\n",
    "        Y_filtered = Y[D==1]\n",
    "        A_filtered = A[D==1]\n",
    "        \n",
    "        \n",
    "        #compute e1 and e0\n",
    "        ey1 = A_filtered*Y_filtered/(ps_filtered * ps_d_filtered) / sum(A_filtered/(ps_filtered * ps_d_filtered))\n",
    "        ey0 = (1-A_filtered)*Y_filtered/((1-ps_filtered) * ps_d_filtered) / sum((1-A_filtered)/((1-ps_filtered) * ps_d_filtered))\n",
    "\n",
    "        ate = ey1.sum()-ey0.sum()\n",
    "        return [ate, ey0.sum(), ey1.sum()] # compute the ATE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverse_event_outcomes = ['AE_c_diff',\n",
    " 'AE_skin', 'AE_GI', 'AE_AKI',\n",
    " 'AE_other']\n",
    "\n",
    "negative_control_outcomes = ['fibro_' + m + '_mon_outcome' for m in ['1','3','6']] + \\\n",
    " ['hernia_' + m + '_mon_outcome' for m in ['1','3','6']] + \\\n",
    " ['fracture_' + m + '_mon_outcome' for m in ['1','3','6']]\n",
    "\n",
    "#removed the below code in new iteration given that feature was added\n",
    "# filtered_cohort['AE_any'] = filtered_cohort[adverse_event_outcomes].any(axis='columns').astype(int)\n",
    "adverse_event_outcomes.append('AE_any')\n",
    "\n",
    "efficacy_outcomes = [f for f in filtered_cohort.columns if f.startswith('t_') and f.endswith('_bin')]\n",
    "efficacy_outcomes = [f for f in efficacy_outcomes if f!='t_condition_occurence_ids']\n",
    "\n",
    "print(adverse_event_outcomes)\n",
    "print(negative_control_outcomes)\n",
    "print(efficacy_outcomes)\n",
    "\n",
    "out_time_dict = {'AE_c_diff' : '90', 'AE_skin' : '30', 'AE_other' : '30', 'AE_GI' : '15', 'AE_AKI' : '30','AE_any':'90'}\n",
    "out_time_dict.update({k: '30' for k in negative_control_outcomes if '1' in k})\n",
    "out_time_dict.update({k: '90' for k in negative_control_outcomes if '3' in k})\n",
    "out_time_dict.update({k: '90' for k in negative_control_outcomes if '6' in k})\n",
    "out_time_dict.update({k: '30' for k in ['t_bin', 't_uti_bin', 't_neph_bin', 't_sepsis_bin', 't_i_uti_bin', 't_i_neph_bin', 't_i_sepsis_bin', 't_i_bin']})\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "sklearn_model_map = {'Logistic_Regression' : LogisticRegression(),\n",
    "     'Random_Forest' : RandomForestClassifier(),\n",
    "     'LGBM' : lgb.LGBMClassifier()}\n",
    "features = [\"cohort_features\",\"omop_1_year_features\"]\n",
    "\n",
    "def update_nested_dict(d, target_key, mapping_dict=sklearn_model_map):\n",
    "    for key, value in d.copy().items():\n",
    "        if key == 'penalty':\n",
    "            if d[key] == 'l1':\n",
    "                d['solver'] = 'liblinear'\n",
    "        if key == target_key:\n",
    "            if d[key] != 'Random_Forest':\n",
    "                d['verbose'] = -1\n",
    "            d[key] = mapping_dict[d[key]]\n",
    "        elif isinstance(value, dict):\n",
    "            d[key] = update_nested_dict(value, target_key)\n",
    "    return d\n",
    "\n",
    "# def add_penalty_params()\n",
    "\n",
    "def format_parameter_dictionaries(p_alt,p_sec):\n",
    "    treatments = ['alternatives','second_line']\n",
    "    \n",
    "    \n",
    "    ## create A params\n",
    "    best_A_params = dict()\n",
    "    best_A_params['alternatives'] = p_alt['alternatives']['alternatives']\n",
    "    best_A_params['second_line'] = p_sec['second_line']['second_line']\n",
    "\n",
    "    \n",
    "    #create D params\n",
    "    best_D_params = dict()\n",
    "    for p_group,t_group in zip([p_alt,p_sec],treatments):\n",
    "        best_D_params[t_group] = {treat:params for treat,params in p_group[t_group].items() if treat!= t_group}\n",
    "\n",
    "    \n",
    "    #convert the clf keys into correct values\n",
    "    best_A_params = update_nested_dict(best_A_params,'clf')\n",
    "    best_D_params = update_nested_dict(best_D_params,'clf')\n",
    "    return best_A_params,best_D_params\n",
    "\n",
    "\n",
    "\n",
    "alternatives = pd.read_pickle(\"../../Data/best_alternatives_hyperparameters.pkl\")\n",
    "second_line = pd.read_pickle(\"../../Data/best_second_line_hyperparameters.pkl\")\n",
    "\n",
    "best_A_params_cohort,best_D_params_cohort = format_parameter_dictionaries(alternatives['cohort_features'],second_line['cohort_features'])\n",
    "\n",
    "best_A_params_omop,best_D_params_omop = format_parameter_dictionaries(alternatives['omop_1_year_features'],second_line['omop_1_year_features'])\n",
    "\n",
    "ATE_A = {\"cohort_features\":best_A_params_cohort, \"omop_1_year_features\":best_A_params_omop}\n",
    "ATE_D = {\"cohort_features\":best_D_params_cohort, \"omop_1_year_features\":best_D_params_omop}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03042024 results (using bootstrap=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cohort.antibiotic_type.value_counts().sum() - 21140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cohort.antibiotic_type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "discard = True\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            print(len(ps))\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            print(len(kp[0]))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in negative_control_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['negative_controls'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "discard = True\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in negative_control_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=1000, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=1000, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['negative_controls'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "discard = True\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in negative_control_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['negative_controls'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results_df,open(f'final_{treatment_name}_03042024_results.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup config from grid search\n",
    "#cohort\n",
    "best_A_params_cohort = {'alternatives':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.1, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},'second_line':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 50, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}} # best params for second line change for alternatives\n",
    "best_D_params_cohort = {'alternatives':{'15': {'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 140, \"clf\":RandomForestClassifier()},\n",
    "                                 '30':{'C': 0.25, 'penalty': 'l1',\"solver\":'liblinear', 'verbose': -1, \"clf\":LogisticRegression()},\n",
    "                                 '90':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}},\n",
    "                'second_line':{'15':{'boosting_type': 'gbdt', 'learning_rate': 0.01, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},\n",
    "                               '30': {'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},\n",
    "                               '90':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}}\n",
    "                }\n",
    "features = [\"cohort_features\",\"omop_1_year_features\"]\n",
    "\n",
    "#omop\n",
    "\n",
    "best_A_params_omop = {'alternatives':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 50, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},'second_line':{'random_state': 0, 'boosting_type': 'dart', 'learning_rate': 0.1, 'num_leaves': 50, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}} # best params for second line change for alternatives\n",
    "best_D_params_omop = {'alternatives':{'15':{'max_features': 'auto', 'min_samples_leaf': 50, 'n_estimators': 100, \"clf\":RandomForestClassifier()},\n",
    "                                 '30':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 250, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},\n",
    "                                 '90':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 140, \"clf\":RandomForestClassifier()}},\n",
    "                'second_line':{'15':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 120, \"clf\":RandomForestClassifier()},\n",
    "                               '30':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 100, \"clf\":RandomForestClassifier()},\n",
    "                               '90':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 120, \"clf\":RandomForestClassifier()}}\n",
    "                }\n",
    "                      \n",
    "ATE_A = {\"cohort_features\":best_A_params_cohort, \"omop_1_year_features\":best_A_params_omop}\n",
    "ATE_D = {\"cohort_features\":best_D_params_cohort, \"omop_1_year_features\":best_D_params_omop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup config from grid search\n",
    "#cohort\n",
    "best_A_params_cohort = {'alternatives':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.1, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},'second_line':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.1, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}} # best params for second line change for alternatives\n",
    "best_D_params_cohort = {'alternatives':{'15': {'max_features': 'auto', 'min_samples_leaf': 50, 'n_estimators': 140, \"clf\":RandomForestClassifier()},\n",
    "                                 '30':{'max_features': 'auto', 'min_samples_leaf': 50, 'n_estimators': 60, \"clf\":RandomForestClassifier()},\n",
    "                                 '90':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}},\n",
    "                'second_line':{'15':{'C': 0.1, 'penalty': 'l1',\"solver\":'liblinear', 'verbose': -1, \"clf\":LogisticRegression()},\n",
    "                               '30': {'boosting_type': 'dart', 'learning_rate': 0.1, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},\n",
    "                               '90':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 10, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}}\n",
    "                }\n",
    "features = [\"cohort_features\",\"omop_1_year_features\"]\n",
    "\n",
    "#omop\n",
    "\n",
    "best_A_params_omop = {'alternatives':{'random_state': 0, 'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 50, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},'second_line':{'random_state': 0, 'boosting_type': 'dart', 'learning_rate': 0.1, 'num_leaves': 50, 'verbose': -1, \"clf\":lgb.LGBMClassifier()}} # best params for second line change for alternatives\n",
    "best_D_params_omop = {'alternatives':{'15':{'max_features': 'auto', 'min_samples_leaf': 50, 'n_estimators': 100, \"clf\":RandomForestClassifier()},\n",
    "                                 '30':{'boosting_type': 'gbdt', 'learning_rate': 0.05, 'num_leaves': 250, 'verbose': -1, \"clf\":lgb.LGBMClassifier()},\n",
    "                                 '90':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 140, \"clf\":RandomForestClassifier()}},\n",
    "                'second_line':{'15':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 120, \"clf\":RandomForestClassifier()},\n",
    "                               '30':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 100, \"clf\":RandomForestClassifier()},\n",
    "                               '90':{'max_features': 'auto', 'min_samples_leaf': 75, 'n_estimators': 120, \"clf\":RandomForestClassifier()}}\n",
    "                }\n",
    "                      \n",
    "ATE_A = {\"cohort_features\":best_A_params_cohort, \"omop_1_year_features\":best_A_params_omop}\n",
    "ATE_D = {\"cohort_features\":best_D_params_cohort, \"omop_1_year_features\":best_D_params_omop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOAH REPLICATION OF MING-CHIEH RESULTS FOR SECOND-LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have to remove the first feature here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing cohort features.  12-14 NOAH COMMENTED THIS OUT BECAUSE I THINK FOR OMOP IT IS PROBABLY NEEDED\n",
    "# features.pop(1)\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 (start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 (end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 (start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2 (end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have to reinitialize filtered_cohort to use the omp version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in order to get it to work, you need to make sure that filtered_cohort is the one from omop.  I think the notebook would benefit from changing that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in negative_control_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['negative_controls'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD MING-CHIEH RESULTS FOR ALTERNATIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "results_df = defaultdict(dict)\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in adverse_event_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['adverse_events'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "#run bootstrap\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in negative_control_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['negative_controls'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discard = True\n",
    "\n",
    "A = outcome_dict['treatment'][treatment_name]\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "    results = pd.DataFrame(columns=['Outcome', 'E[Y_1]', 'E[Y_0]', 'ATE', '95% C.I.'])\n",
    "    for y in efficacy_outcomes:\n",
    "        X = feat_dict['treatment'][feature][0]\n",
    "        time = out_time_dict[y]\n",
    "        D = outcome_dict['censor'][time]\n",
    "        params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "        if discard:\n",
    "            ps = run_ps3((X, A, filtered_cohort[y], D), params, prop = True)\n",
    "            kp = np.where((ps >= 0.05) * (ps <= 0.95))\n",
    "            temp_filtered_cohort = filtered_cohort.copy()\n",
    "            filtered_cohort = filtered_cohort.iloc[kp]\n",
    "            params = (ATE_A[feature][treatment_name], ATE_D[feature][treatment_name][time])\n",
    "            summary = censor_bootstrap(X[kp], A.iloc[kp], y, D.iloc[kp], bootstrap_sample=100, other_word=y, params=params)\n",
    "            filtered_cohort = temp_filtered_cohort.copy()\n",
    "        else:\n",
    "            summary = censor_bootstrap(X, A, y, D, bootstrap_sample=100, other_word=y, params=params)\n",
    "        results.loc[len(results)] = [y] + summary\n",
    "    \n",
    "    results_df['t_efficacy'][feature] = results\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['adverse_events']['cohort_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['adverse_events']['omop_1_year_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['negative_controls']['cohort_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['negative_controls']['omop_1_year_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['t_efficacy']['cohort_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['t_efficacy']['omop_1_year_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [\"adverse_events\",\"t_efficacy\"]\n",
    "feats = [\"cohort_features\",\"omop_1_year_features\"]\n",
    "\n",
    "out_gname = {\"adverse_events\":\"adverse_events\",\"t_efficacy\":\"treatment efficacy\"}\n",
    "for o in outcomes:\n",
    "    for f in feats:\n",
    "        outcome_group_name = out_gname[o]\n",
    "        results_df[o][f].to_pickle('Aggregate_results/'+f+'_'+treatment+'_'+outcome_group_name+'_'+'censor_v0'+'.pkl')\n",
    "        results\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = run_lgbm(feat_dict[final_feature][0],outcome_dict['treatment'][treatment_name],add_propensity=True,params=best_params,search=False)\n",
    "cohort, clf = lgbm[\"cohort\"],lgbm[\"classifier\"]\n",
    "df_coef = pd.DataFrame(list(zip(feat_dict[final_feature][1], clf.feature_importances_)), columns=['feature', 'lgbm importance'])\n",
    "print(df_coef.shape)\n",
    "display(df_coef.iloc[(-df_coef['lgbm importance'].abs()).argsort()].head(30).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_A_params_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict['treatment']['cohort_features'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_A_params_cohort[treatment_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = run_lgbm(feat_dict['treatment']['cohort_features'][0], outcome_dict['treatment'][treatment_name], add_propensity = True, params = best_A_params_cohort[treatment_name], search = False) \n",
    "clf = lgbm[1]\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm = run_lgbm(feat_dict['treatment']['cohort_features'][0], outcome_dict['treatment'][treatment_name], add_propensity = True, params = best_A_params_cohort[treatment_name], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "cols = feat_dict['treatment']['cohort_features'][1]\n",
    "replace_list = [\\\n",
    "     ('age', 'Age'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('specialty_advanced_specialist_group', 'Advanced specialist visit'),\\\n",
    "     ('alternatives_12_to_24_mo','Alternative antibiotics in 12 to 24 months'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('alternatives_6_to_12_mo','Alternative antibiotics in 6 to 12 months'),\\\n",
    "     ('specialty_internal_medicine_group', 'Internalist visit'),\\\n",
    "     ('specialty_OBGYN_group', 'Obstetricist/Gyencologist visit'),\\\n",
    "     ('specialty_other_group', 'Physician specialty other than prespecified groups*'),\\\n",
    "     ('alternatives_0_to_6_mo','Alternative antibiotics in 6 months'),\\\n",
    "     ('years_since_diagnosis', 'Prescription year'),\\\n",
    "     ('specialty_advanced_specialist_group', 'Advanced specialist visit'),\\\n",
    "     ('second_line_12_to_24_mo','Second-line antibiotics in 12 to 24 months'),\\\n",
    "     ('nitrofurantoin_12_to_24_mo','Nitrofurantoin in 12 to 24 months'),\\\n",
    "     ('specialty_internal_medicine_group', 'Internalist visit'),\\\n",
    "     ('second_line_6_to_12_mo','Second-line antibiotics in 6 to 12 months'),\\\n",
    "     ('urine_test_present', 'Urine test ordered'),\\\n",
    "     ('specialty_OBGYN_group', 'Obstetricist/Gyencologist visit'),\\\n",
    "     ('second_line_most_recent', 'Most recent antibiotic is second-line'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('days_since_previous_uti', 'Days since previous UTI'),\\\n",
    "     ('previous_utis', 'UTI history within 2 years'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('trimethoprim-sulfamethoxazole_most_recent', 'Most recent antibiotic is TMP-SMX'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('ph_urine_0', 'Urine pH value'),\\\n",
    "     ('creatinine_0', 'Creatinine value'),\\\n",
    "     ('neutrophil_p100_0', 'Neutrophil/100 leukocytes in blood value'),\\\n",
    "    ('diabetes_mellitus0_6_months','Diabetes mellitus in 6 months'),\\\n",
    "    ('hypertension0_6_months','Hypertension in 6 months'),\\\n",
    "     ('urine_test_present', 'Urine test ordered'),\\\n",
    "     ('years_since_diagnosis', 'Prescription year'),\\\n",
    "     ('treatment_','Alternative antibiotics prescribed')]\n",
    "for i in replace_list:\n",
    "     cols = [s.replace(i[0], i[1]) for s in cols]\n",
    "X_df =  pd.DataFrame(feat_dict['treatment']['cohort_features'][0].todense(),\\\n",
    "     columns = cols)\n",
    "shap_values = explainer.shap_values(X_df)\n",
    "shap.summary_plot(shap_values, X_df, max_display = 10, class_names = ['First-line','Second-line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm = run_lgbm(feat_dict['treatment']['cohort_features'][0], outcome_dict['treatment'][treatment_name], add_propensity = True, params = best_A_params_cohort[treatment_name], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "cols = feat_dict['treatment']['cohort_features'][1]\n",
    "replace_list = [\\\n",
    "     ('age', 'Age'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('specialty_advanced_specialist_group', 'Advanced specialist visit'),\\\n",
    "     ('alternatives_12_to_24_mo','Alternative antibiotics in 12 to 24 months'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('alternatives_6_to_12_mo','Alternative antibiotics in 6 to 12 months'),\\\n",
    "     ('specialty_internal_medicine_group', 'Internalist visit'),\\\n",
    "     ('specialty_OBGYN_group', 'Obstetricist/Gyencologist visit'),\\\n",
    "     ('specialty_other_group', 'Physician specialty other than prespecified groups*'),\\\n",
    "     ('alternatives_0_to_6_mo','Alternative antibiotics in 6 months'),\\\n",
    "     ('years_since_diagnosis', 'Prescription year'),\\\n",
    "     ('specialty_advanced_specialist_group', 'Advanced specialist visit'),\\\n",
    "     ('second_line_12_to_24_mo','Second-line antibiotics in 12 to 24 months'),\\\n",
    "     ('nitrofurantoin_12_to_24_mo','Nitrofurantoin in 12 to 24 months'),\\\n",
    "     ('specialty_internal_medicine_group', 'Internalist visit'),\\\n",
    "     ('second_line_6_to_12_mo','Second-line antibiotics in 6 to 12 months'),\\\n",
    "     ('urine_test_present', 'Urine test ordered'),\\\n",
    "     ('specialty_OBGYN_group', 'Obstetricist/Gyencologist visit'),\\\n",
    "     ('second_line_most_recent', 'Most recent antibiotic is second-line'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('days_since_previous_uti', 'Days since previous UTI'),\\\n",
    "     ('previous_utis', 'UTI history within 2 years'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('trimethoprim-sulfamethoxazole_most_recent', 'Most recent antibiotic is TMP-SMX'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('ph_urine_0', 'Urine pH value'),\\\n",
    "     ('creatinine_0', 'Creatinine value'),\\\n",
    "     ('neutrophil_p100_0', 'Neutrophil/100 leukocytes in blood value'),\\\n",
    "    ('diabetes_mellitus0_6_months','Diabetes mellitus in 6 months'),\\\n",
    "    ('hypertension0_6_months','Hypertension in 6 months'),\\\n",
    "     ('urine_test_present', 'Urine test ordered'),\\\n",
    "     ('years_since_diagnosis', 'Prescription year'),\\\n",
    "     ('treatment_','Alternative antibiotics prescribed')]\n",
    "for i in replace_list:\n",
    "     cols = [s.replace(i[0], i[1]) for s in cols]\n",
    "X_df =  pd.DataFrame(feat_dict['treatment']['cohort_features'][0].todense(),\\\n",
    "     columns = cols)\n",
    "shap_values = explainer.shap_values(X_df)\n",
    "shap.summary_plot(shap_values[1],X_df, max_display = 10,  plot_type=\"dot\", class_names = ['First-line','Second-line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_df, max_display = 10, class_names = ['First-line','Second-line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm = run_lgbm(feat_dict['censor']['cohort_features'][0], outcome_dict['censor']['30'], add_propensity = True, params = best_D_params_cohort[treatment_name]['30'], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "cols = feat_dict['censor']['cohort_features'][1]\n",
    "replace_list = [\\\n",
    "     ('days_since_previous_uti', 'Days since previous UTI'),\\\n",
    "     ('age', 'Age'),\\\n",
    "     ('previous_utis', 'UTI history with 2 years'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('trimethoprim-sulfamethoxazole_most_recent', 'Most recent antibiotics is TMP-SMX'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('ph_urine_0', 'Urine pH value'),\\\n",
    "     ('urine_test_present', 'Urine test ordered'),\\\n",
    "     ('years_since_diagnosis', 'Prescription year'),\\\n",
    "     ('treatment_','Alternative antibiotics prescribed')]\n",
    "for i in replace_list:\n",
    "     cols = [s.replace(i[0], i[1]) for s in cols]\n",
    "X_df =  pd.DataFrame(feat_dict['censor']['cohort_features'][0].todense(),\\\n",
    "     columns = cols)\n",
    "shap_values = explainer.shap_values(X_df)\n",
    "shap.summary_plot(shap_values, X_df, max_display = 10, class_names = ['Censored','Not censored'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lgbm = run_lgbm(feat_dict['treatment']['cohort_features'][0], outcome_dict['treatment'][treatment_name], add_propensity = True, params = best_A_params_cohort[treatment_name][', search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "cols = feat_dict['treatment']['cohort_features'][1]\n",
    "replace_list = [\\\n",
    "     ('age', 'Age'),\\\n",
    "     ('specialty_emergency/acute_group', 'Emergency medicine visit'),\\\n",
    "     ('specialty_advanced_specialist_group', 'Advanced specialist visit'),\\\n",
    "     ('alternatives_12_to_24_mo','Alternative antibiotics in 12 to 24 months'),\\\n",
    "     ('specialty_family_medicine_group', 'Family medicine visit'),\\\n",
    "     ('alternatives_6_to_12_mo','Alternative antibiotics in 6 to 12 months'),\\\n",
    "     ('specialty_internal_medicine_group', 'Internalist visit'),\\\n",
    "     ('specialty_OBGYN_group', 'Obstetricist/Gyencologist visit'),\\\n",
    "     ('specialty_other_group', 'Physician specialty other than prespecified groups*'),\\\n",
    "     ('alternatives_0_to_6_mo','Alternative antibiotics in 6 months')]\n",
    "for i in replace_list:\n",
    "     cols = [s.replace(i[0], i[1]) for s in cols]\n",
    "X_df =  pd.DataFrame(feat_dict['treatment']['cohort_features'][0].todense(),\\\n",
    "     columns = cols)\n",
    "shap_values = explainer.shap_values(X_df)\n",
    "shap.summary_plot(shap_values, X_df, max_display = 10, class_names = ['First-line','Alternatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols[50:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "condition_name = 'Second Line'\n",
    "\n",
    "\n",
    "def calibration(cohort,y_pred,y_col,title=None,lim=None,cond_name=None,name=None,show=False):\n",
    "    '''\n",
    "    Input: cohort with a propensity column named propensity_score and antibiotic type column\n",
    "           treatment name\n",
    "    \n",
    "    return plot'''\n",
    "   \n",
    "    bins = [np.quantile(cohort[y_pred], x/10) for x in range(11)]\n",
    "    df_cali = pd.DataFrame(columns=['pred', 'actual', 'errorbar'])\n",
    "    for i in range(10):\n",
    "        temp = cohort[(cohort[y_pred]>=bins[i]) & (cohort[y_pred]<=bins[i+1])]\n",
    "        x = np.mean(temp[y_pred])\n",
    "        y = len(temp[temp[y_col]==1])/len(temp)\n",
    "        std = np.std(temp[y_pred])\n",
    "        df_cali.loc[len(df_cali)] = [x,y,1.96*std]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    # plt.plot(df_cali.pred, df_cali.actual, '-o')\n",
    "    plt.errorbar(df_cali.pred, df_cali.actual, yerr=df_cali.errorbar, fmt='o', ls='--',color='black')\n",
    "    plt.plot([0, 1], [0, 1], ls='--',color='black')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Observed Probability')\n",
    "    if lim is not None:\n",
    "        plt.xlim(None,lim)\n",
    "        plt.ylim(None,lim)\n",
    "        \n",
    "    if cond_name is None:\n",
    "        plt.title('Calibration Plot for '+treatment)\n",
    "    else:\n",
    "        plt.title('Calibration Plot for '+cond_name)\n",
    "    if name:\n",
    "        plt.savefig(name)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "calibration(cohort,y_pred=y_pred_column,y_col=y_column,cond_name=condition_name,lim=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Sample data setup\n",
    "# df = pd.DataFrame({'y_pred': [0.1, 0.4, 0.35, 0.8, 0.7, 0.2, 0.5, 0.6, 0.9, 0.75], 'y': [0, 0, 1, 1, 1, 0, 1, 1, 1, 0]})\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(df['y'], df['y_pred'], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(df['y_pred'], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = np.sqrt(true_prob * (1 - true_prob) / bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "ci_upper = true_prob + z * se\n",
    "\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=z * se, fmt='o-', color='b', label='Calibration plot w/ 95% CI', linewidth=1, markersize=5)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfectly calibrated')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration Plot with 95% Confidence Intervals')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(bin_sizes==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "condition_name = 'Second Line'\n",
    "\n",
    "def get_se(true_prob,bin_sizes):\n",
    "    updated_true_prob = np.insert(true_prob,np.argwhere(bin_sizes==0)[0],np.nan)\n",
    "    se = np.sqrt(updated_true_prob * (1 - updated_true_prob) / bin_sizes)\n",
    "    se = se[~np.isnan(se)]\n",
    "    return se\n",
    "\n",
    "_,_,_, T_Test = train_test_split(cohort, cohort[y_column], test_size = 0.2, random_state = 0)\n",
    "reduced_cohort = cohort.iloc[T_Test.index]\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(reduced_cohort[y_column], reduced_cohort[y_pred_column], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(reduced_cohort[y_pred_column], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = get_se(true_prob,bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "upper = (true_prob + z * se)\n",
    "upper[np.argwhere(true_prob+z*se > 1)] = 1\n",
    "y_err = [z*se,(upper -true_prob)]\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=y_err,fmt='o--', color='b', label='Calibration curve & 95% Confidence Interval')\n",
    "# plt.fill_between(pred_prob, (true_prob - z*se), upper, color='b', alpha=.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfectly calibrated')\n",
    "plt.xlabel('Predicted probability',fontsize=14)\n",
    "plt.ylabel('Observed Probability',fontsize=14)\n",
    "plt.title(f'Calibration Plot for {condition_name}',fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_A_params_cohort[treatment_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_D_params_cohort[treatment_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "\n",
    "lgbm = run_lgbm(feat_dict['treatment']['cohort_features'][0], outcome_dict['treatment'][treatment_name], add_propensity = True, params = best_A_params_cohort[treatment_name], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "print(f\"Test AUROC is: {lgbm[4][1]}\")\n",
    "def get_se(true_prob,bin_sizes):\n",
    "    \n",
    "    updated_true_prob = true_prob\n",
    "    for idx in np.where(bin_sizes == 0)[0]:\n",
    "        updated_true_prob = np.insert(updated_true_prob, idx, np.nan)\n",
    "    se = np.sqrt(updated_true_prob * (1 - updated_true_prob) / bin_sizes)\n",
    "    se = se[~np.isnan(se)]\n",
    "    return se\n",
    "\n",
    "_,_,_, T_Test = train_test_split(cohort, cohort[y_column], test_size = 0.2, random_state = 0)\n",
    "reduced_cohort = cohort.iloc[T_Test.index]\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(reduced_cohort[y_column], reduced_cohort[y_pred_column], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(reduced_cohort[y_pred_column], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = get_se(true_prob,bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "upper = (true_prob + z * se)\n",
    "upper[np.argwhere(true_prob+z*se > 1)] = 1\n",
    "y_err = [z*se,(upper -true_prob)]\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=y_err,fmt='o--', color='black', label='Calibration curve & 95% Confidence Interval')\n",
    "# plt.fill_between(pred_prob, (true_prob - z*se), upper, color='b', alpha=.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect calibration')\n",
    "plt.xlabel('Predicted probability',fontsize=14)\n",
    "plt.ylabel('Observed Probability',fontsize=14)\n",
    "plt.title(f'Calibration Plot for Second-line Propensity Model',fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censor 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "condition_name = '15-Day Censoring'\n",
    "\n",
    "lgbm = run_lgbm(feat_dict['censor']['cohort_features'][0], outcome_dict['censor']['15'], add_propensity = True, params = best_D_params_cohort[treatment_name]['15'], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "print(f\"Test AUROC is: {lgbm[4][1]}\")\n",
    "def get_se(true_prob,bin_sizes):\n",
    "    \n",
    "    updated_true_prob = true_prob\n",
    "    for idx in np.where(bin_sizes == 0)[0]:\n",
    "        updated_true_prob = np.insert(updated_true_prob, idx, np.nan)\n",
    "    se = np.sqrt(updated_true_prob * (1 - updated_true_prob) / bin_sizes)\n",
    "    se = se[~np.isnan(se)]\n",
    "    return se\n",
    "\n",
    "_,_,_, T_Test = train_test_split(cohort, cohort[y_column], test_size = 0.2, random_state = 0)\n",
    "reduced_cohort = cohort.iloc[T_Test.index]\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(reduced_cohort[y_column], reduced_cohort[y_pred_column], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(reduced_cohort[y_pred_column], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = get_se(true_prob,bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "upper = (true_prob + z * se)\n",
    "upper[np.argwhere(true_prob+z*se > 1)] = 1\n",
    "y_err = [z*se,(upper -true_prob)]\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=y_err,fmt='o--', color='black', label='Calibration curve & 95% Confidence Interval')\n",
    "# plt.fill_between(pred_prob, (true_prob - z*se), upper, color='b', alpha=.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect calibration')\n",
    "plt.xlabel('Predicted probability',fontsize=14)\n",
    "plt.ylabel('Observed Probability',fontsize=14)\n",
    "plt.title(f'Calibration Plot for Second-line {condition_name} Model',fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censor 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict['censor']['30'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "condition_name = '30-Day Censoring'\n",
    "\n",
    "logreg = run_logreg(feat_dict['censor']['cohort_features'][0], outcome_dict['censor']['30'], add_propensity = True, params = best_D_params_cohort[treatment_name]['30'], search = False) \n",
    "\n",
    "cohort, clf = logreg[0],logreg[1]\n",
    "print(f\"Test AUROC is: {logreg[4][1]}\")\n",
    "def get_se(true_prob,bin_sizes):\n",
    "    \n",
    "    updated_true_prob = true_prob\n",
    "    for idx in np.where(bin_sizes == 0)[0]:\n",
    "        updated_true_prob = np.insert(updated_true_prob, idx, np.nan)\n",
    "    se = np.sqrt(updated_true_prob * (1 - updated_true_prob) / bin_sizes)\n",
    "    se = se[~np.isnan(se)]\n",
    "    return se\n",
    "\n",
    "_,_,_, T_Test = train_test_split(cohort, cohort[y_column], test_size = 0.2, random_state = 0)\n",
    "reduced_cohort = cohort.iloc[T_Test.index]\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(reduced_cohort[y_column], reduced_cohort[y_pred_column], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(reduced_cohort[y_pred_column], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = get_se(true_prob,bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "upper = (true_prob + z * se)\n",
    "upper[np.argwhere(true_prob+z*se > 1)] = 1\n",
    "y_err = [z*se,(upper -true_prob)]\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=y_err,fmt='o--', color='black', label='Calibration curve & 95% Confidence Interval')\n",
    "# plt.fill_between(pred_prob, (true_prob - z*se), upper, color='b', alpha=.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect calibration')\n",
    "plt.xlabel('Predicted probability',fontsize=14)\n",
    "plt.ylabel('Observed Probability',fontsize=14)\n",
    "plt.title(f'Calibration Plot for Second-line {condition_name} Model',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy.stats import norm\n",
    "y_pred_column = 'propensity_score'\n",
    "y_column = 'treatment'\n",
    "condition_name = '90-Day Censoring'\n",
    "\n",
    "lgbm = run_lgbm(feat_dict['censor']['cohort_features'][0], outcome_dict['censor']['90'], add_propensity = True, params = best_D_params_cohort[treatment_name]['90'], search = False) \n",
    "\n",
    "cohort, clf = lgbm[0],lgbm[1]\n",
    "print(f\"Test AUROC is: {lgbm[4][1]}\")\n",
    "def get_se(true_prob,bin_sizes):\n",
    "    \n",
    "    updated_true_prob = true_prob\n",
    "    for idx in np.where(bin_sizes == 0)[0]:\n",
    "        updated_true_prob = np.insert(updated_true_prob, idx, np.nan)\n",
    "    se = np.sqrt(updated_true_prob * (1 - updated_true_prob) / bin_sizes)\n",
    "    se = se[~np.isnan(se)]\n",
    "    return se\n",
    "\n",
    "_,_,_, T_Test = train_test_split(cohort, cohort[y_column], test_size = 0.2, random_state = 0)\n",
    "reduced_cohort = cohort.iloc[T_Test.index]\n",
    "\n",
    "# Generate the calibration data\n",
    "true_prob, pred_prob = calibration_curve(reduced_cohort[y_column], reduced_cohort[y_pred_column], n_bins=10, strategy='uniform')\n",
    "\n",
    "# Calculate bin sizes\n",
    "bin_sizes = np.histogram(reduced_cohort[y_pred_column], bins=10, range=(0, 1))[0]\n",
    "\n",
    "# Calculate standard error for the binomial proportion in each bin\n",
    "se = get_se(true_prob,bin_sizes)\n",
    "\n",
    "# Calculate the z-value for the 95% confidence interval\n",
    "z = norm.ppf(0.975)  # Two-tailed, hence 0.975 instead of 0.95\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "ci_lower = true_prob - z * se\n",
    "upper = (true_prob + z * se)\n",
    "upper[np.argwhere(true_prob+z*se > 1)] = 1\n",
    "y_err = [z*se,(upper -true_prob)]\n",
    "# Plotting the calibration curve with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(pred_prob, true_prob, yerr=y_err,fmt='o--', color='black', label='Calibration curve & 95% Confidence Interval')\n",
    "# plt.fill_between(pred_prob, (true_prob - z*se), upper, color='b', alpha=.1)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Perfect calibration')\n",
    "plt.xlabel('Predicted probability',fontsize=14)\n",
    "plt.ylabel('Observed Probability',fontsize=14)\n",
    "plt.title(f'Calibration Plot for Second-line {condition_name} Model',fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop_v2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "872c36992472fec203c4024a07af22780f326cb1d5c9da900b82ffb7760bf3eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
